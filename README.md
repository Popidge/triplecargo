â™ ï¸ Triplecargo â€” Triple Triad Assistant & Solver (FF8)


Deterministic, highâ€‘performance Rust implementation of the Final Fantasy VIII Triple Triad ruleset, designed to support:


- Perfect play solving (Negamax + Î±Î² + TT).
- Full stateâ€‘space precomputation for instant queries.
- Analysis experiments (firstâ€‘player advantage, card ratings, rule impacts).
- AI/ML integration for lightweight move suggestion and imperfectâ€‘information play.
Triplecargo is both a researchâ€‘grade solver and the foundation for a superhuman Triple Triad assistant â€” combining exact computation with modern AI techniques.


---

âœ¨ Features

- Full ruleset: Basic, Elemental, Same, Plus, Same Wall, Combo cascades.
- Deterministic engine: fixed move ordering, reproducible cascades, stable hashing.
- Highâ€‘performance solver: Negamax + Î±Î² pruning + transposition table.
- Precompute driver: bulk solve all reachable states for a given hand/ruleset.
- Persistence: appendâ€‘only batch writes, deterministic compaction, optional compression, checkpoint/resume.
- Analysis mode: RAMâ€‘only runs for experiments (no I/O bottleneck).
- Throughput: ~10M nodes/sec, ~1M states/sec on a Ryzen 3600.
- Training data export (JSONL): trajectory or full stateâ€‘space, policy_format onehot|mcts, deterministic with fixed seed.

---

ğŸ“‚ Project layout

- Core engine: rules, state, hashing, scoring.
- Solver: negamax + Î±Î² + TT, PV reconstruction.
- Precompute: state enumeration, parallel solve, persistence.
- Persistence: batch writes, compression, checkpointing.
- Binaries:
	- tt-cli: demo CLI.
	- precompute: bulk solver + DB writer.
- Tests: engine correctness, rule edge cases, solver determinism.

---

ğŸ§® Data model

- Cards: loaded from data/cards.json, validated for ranges and uniqueness.
- Board: 3Ã—3 fixed array, optional perâ€‘cell element.
- Hands: 5 cards per player (engine: fixed arrays; export: shrinking lists as cards are played).
- Rules: independent toggles for Elemental, Same, Plus, Same Wall.
- Hashing: XORâ€‘based Zobrist, incremental updates, 128â€‘bit keys.

---
 
ğŸ“¤ Training data export (JSONL)

- Modes (via precompute):
  - --export-mode trajectory (default): exports a single 9â€‘state principalâ€‘variation trajectory per sampled game.
  - --export-mode full: exports the entire reachable state space for one sampled hand pair (exhaustive).

- Determinism:
  - --seed controls sampling (hands/elements) and, if mcts is selected, the rollout RNG.
  - Same seed + flags â†’ identical JSONL output (including Elemental layout and policy targets).

- Value targets:
  - --value-mode winloss (default): sign(value) from sideâ€‘toâ€‘move perspective âˆˆ {-1, 0, +1}.
  - --value-mode margin: final Aâ€‘perspective score margin (A_cards âˆ’ B_cards) âˆˆ [-9, +9].

- Hand sampling:
  - --hand-strategy random: uniform without replacement from all 110 cards.
  - --hand-strategy stratified: one card from each level band [1â€“2], [3â€“4], [5â€“6], [7â€“8], [9â€“10].

- Elements:
  - --elements none|random with deterministic perâ€‘game element RNG when random is chosen.
  - Element letter codes on export: F (Fire), I (Ice), T (Thunder), W (Water), E (Earth), P (Poison), H (Holy), L (Wind).

- Policy targets:
  - --policy-format onehot (default): exported as a single move object {"card_id":..., "cell":...}.
  - --policy-format mcts: exported as a {"cardId-cell": prob} distribution over legal moves; --mcts-rollouts N (default 100) controls simulations.
  - In terminal states: onehot â†’ policy_target omitted (null), mcts â†’ {} empty map.

- JSONL writing:
  - Lines are appended as they are generated (streaming), with periodic flushes; no buffering of entire games.

CLI examples
- Trajectory export (onehot), 10 games, no elements:
  - target/release/precompute --export export.jsonl --export-mode trajectory --games 10 --seed 42 --hand-strategy random --rules none --elements none --policy-format onehot --value-mode winloss
- Trajectory export (MCTS with 256 rollouts), stratified hands, with Elemental+Same and margin targets:
  - target/release/precompute --export export_mcts.jsonl --export-mode trajectory --games 10 --seed 42 --hand-strategy stratified --rules elemental,same --elements random --policy-format mcts --mcts-rollouts 256 --value-mode margin
- Full stateâ€‘space export (exhaustive) for one sampled hand pair:
  - target/release/precompute --export export_full.jsonl --export-mode full --seed 42 --hand-strategy random --rules none --elements none --policy-format onehot --value-mode winloss

JSONL schema
Each line contains one state. Fields:

{
  "game_id": 12,                // sequential per sampled game (trajectory); full mode uses 0
  "state_idx": 0,               // 0..8 within a game trajectory (equals "turn")
  "board": [
    {"cell":0,"card_id":12,"owner":"A","element":"F"}, // element present only when rules.elemental=true; otherwise omitted
    {"cell":1,"card_id":null,"owner":null,"element":null},
    {"cell":2,"card_id":null,"owner":null,"element":"W"}
  ],
  "hands": {
    "A": [34,56,78,90,11],      // shrinking lists (5 â†’ 0 across a trajectory)
    "B": [22,33,44,55,66]
  },
  "to_move": "A",
  "turn": 0,
  "rules": {
    "elemental": true,
    "same": true,
    "plus": false,
    "same_wall": false
  },

  // Policy target semantics:
  // onehot â†’ single best move object:
  //   "policy_target": {"card_id":34,"cell":0}
  // mcts â†’ distribution over legal moves:
  //   "policy_target": {"34-0":0.7,"56-1":0.3}
  "policy_target": {"34-0": 0.7, "56-1": 0.3},

  // Value target semantics controlled by --value-mode:
  // - winloss â†’ {-1,0,+1} from side-to-move perspective (sign of solver value)
  // - margin  â†’ integer margin A_cards âˆ’ B_cards (A-perspective)
  "value_target": 1,
  "value_mode": "winloss",

  // Off-principal-variation sampling flag; true for all lines of off-PV games, false otherwise
  "off_pv": false,

  // 128-bit Zobrist hash of the state (hex string)
  "state_hash": "a1b2c3d4e5f6a1b2c3d4e5f6a1b2c3d4"
}

Notes
- In trajectory mode, the solver searches full remaining depth at each ply to produce value_target consistent with final outcome.
- In full mode, enumeration exports all reachable states for the single sampled hand pair (useful for analysis). For mcts in full mode, rollout RNG is derived from (seed XOR state Zobrist) for traversalâ€‘order invariance.

ğŸ“‘ Export Semantics

- 
State timing


	- Each JSONL line represents the state before a move is made.
	- At turn = t, the board has t occupied cells, and the side in to_move is about to play their (t+1)â€‘th card.
	- Hands shrink accordingly, but at the final turn = 8 there is still one card left in the moverâ€™s hand and a policy_target showing where it will be placed.
- 
Policy targets


	- --policy-format onehot:
		- Exported as a single move object:

	"policy_target": {"card_id": 34, "cell": 0}



	- --policy-format mcts:
		- Exported as a distribution over legal moves:

	"policy_target": {"34-0": 0.7, "56-1": 0.3}



	- Terminal states:
		- onehot â†’ policy_target omitted (null).
		- mcts â†’ empty map {}.
- 
	- Off-PV stepping (strategy=mcts):
		- When --off-pv-strategy mcts is active:
			- If --policy-format mcts is selected, the root MCTS distribution computed for policy_target is reused for stepping.
			- Otherwise, a root MCTS distribution is computed with --mcts-rollouts at each ply using a deterministic per-ply seed.
			- The PV moveâ€™s probability is set to 0 and the distribution is renormalised; the next move is sampled deterministically via a per-ply RNG tied to (seed, game_id, turn).
			- Fallback: if the PV move had all probability mass (sum of non-PV probs == 0), the highest-probability non-PV move is selected deterministically.
Value targets


	- Controlled by --value-mode:
		- winloss: value âˆˆ {-1, 0, +1}, always from the sideâ€‘toâ€‘move perspective.
			- +1 = sideâ€‘toâ€‘move wins under perfect play.
			- 0 = draw.
			- âˆ’1 = sideâ€‘toâ€‘move loses.
		- margin: integer final score difference (A_cards âˆ’ B_cards), always from Aâ€™s perspective, independent of sideâ€‘toâ€‘move.
	- Values are computed by solving the full remaining depth at each ply, so they are consistent with the final outcome.
- 
Elemental layout


	- When rules.elemental = true, each board cell includes an "element" field with one of:
		- "F" (Fire), "I" (Ice), "T" (Thunder), "W" (Water), "E" (Earth), "P" (Poison), "H" (Holy), "L" (Wind).
	- Otherwise, the "element" field is omitted.
- 
Metadata


	- game_id: sequential per sampled game in trajectory mode; fixed 0 in full mode.
	- state_idx: 0..8 within a trajectory (equals turn).
	- off_pv: boolean flag indicating off-principal-variation sampling; true for all lines of off-PV games, false otherwise.
	- state_hash: 128â€‘bit Zobrist hash of the state, hex string.

ğŸ•¹ï¸ Rules implemented

- Basic capture: placed card flips weaker adjacent opponents.
- Elemental: +1/âˆ’1 adjustments based on cell element.
- Same: 2+ equalities trigger flips.
- Same Wall: walls count as 10 for Same.
- Plus: equal sums trigger flips.
- Combo: cascades apply Basic rule only.

---

âš¡ Performance & determinism

- Move ordering: Corners > Edges > Center, then cell index, then card id.
- Cascade order: BFS queue, ascending indices.
- TT: fixedâ€‘size array, depthâ€‘preferred replacement, full key verification.
- Progress: MultiProgress bars, states/sec, nodes/sec.
- Determinism: same state â†’ same result, reproducible DBs.

---

ğŸ”® Roadmap

Nearâ€‘term

- Query API: CLI/HTTP service for instant bestâ€‘move lookups from solved DBs.
- Analysis experiments:
	- Firstâ€‘player advantage quantification.
	- Card/hand Elo ratings.
	- Elemental variance studies.
	- Rule interaction balance.

Midâ€‘term

- AI/ML integration:
	- NPZ export option for compact storage.
	- PyTorch Dataset utilities to load JSONL â†’ tensors.
	- Train small policy/value nets for lightweight move suggestion.
	- Reinforced selfâ€‘play (AlphaZeroâ€‘lite) to refine policies.
- Imperfectâ€‘info play:
	- Expectimax/MCTS with hidden hands.
	- Opponent modelling.

Longâ€‘term

- Superhuman assistant:
	- Instant move suggestions in Open and Closed formats.
	- Configurable playstyles: Optimal, Robust, Exploitative.
	- Humanâ€‘like play modes for fun.
- Metaâ€‘analysis:
	- â€œBest handâ€ search for each ruleset.
	- Card tier lists and balance insights.
	- Quantitative impact of Elemental RNG.

---

ğŸ§ª Testing scope

- Engine tests: rule correctness, determinism, allocationâ€‘free hot paths.
- Solver tests: terminal values, PV determinism.
- Rule edge tests: Same, Plus, Same Wall, Combo cascades.
- Persistence tests: batch flush, compression, checkpoint/resume determinism.

---

ğŸ“œ License


The code in this repository is provided under an open license (TBD).
Triple Triad belongs to Square Enix (Final Fantasy VIII). This is a cleanâ€‘room reimplementation for research and educational purposes.


---

ğŸ™ Acknowledgements

- FF8â€™s Triple Triad ruleset by Square Enix.
- Chess/Go engine design patterns (negamax, Î±Î², Zobrist, TT).
- OpenAI GPTâ€‘5 + Kilo Code for collaborative design and implementation.