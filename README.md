â™ ï¸ Triplecargo â€” Triple Triad Assistant & Solver (FF8)

Deterministic, highâ€‘performance Rust implementation of the Final Fantasy VIII Triple Triad ruleset, designed to support:

- Perfect play solving (Negamax + Î±Î² + TT).
- Full stateâ€‘space precomputation for instant queries.
- Analysis experiments (firstâ€‘player advantage, card ratings, rule impacts).
- AI/ML integration for lightweight move suggestion and imperfectâ€‘information play.
Triplecargo is both a researchâ€‘grade solver and the foundation for a superhuman Triple Triad assistant â€” combining exact computation with modern AI techniques.

---
âœ¨ Features

- Full ruleset: Basic, Elemental, Same, Plus, Same Wall, Combo cascades.
- Deterministic engine: fixed move ordering, reproducible cascades, stable hashing.
- Highâ€‘performance solver: Negamax + Î±Î² pruning + transposition table.
- Precompute driver: bulk solve all reachable states for a given hand/ruleset.
- Persistence: appendâ€‘only batch writes, deterministic compaction, optional compression, checkpoint/resume.
- Analysis mode: RAMâ€‘only runs for experiments (no I/O bottleneck).
- Throughput: ~10M nodes/sec, ~1M states/sec on a Ryzen 3600.
- Training data export (JSONL): trajectory or full stateâ€‘space, policy_format onehot|mcts, deterministic with fixed seed.

---
ğŸ“‚ Project layout

- Core engine: rules, state, hashing, scoring.
- Solver: negamax + Î±Î² + TT, PV reconstruction.
- Precompute: state enumeration, parallel solve, persistence.
- Persistence: batch writes, compression, checkpointing.
- Binaries:
	- tt-cli: demo CLI.
	- precompute: bulk solver + DB writer.
- Tests: engine correctness, rule edge cases, solver determinism.

---
ğŸ§® Data model

- Cards: loaded from data/cards.json, validated for ranges and uniqueness.
- Board: 3Ã—3 fixed array, optional perâ€‘cell element.
- Hands: 5 cards per player (engine: fixed arrays; export: shrinking lists as cards are played).
- Rules: independent toggles for Elemental, Same, Plus, Same Wall.
- Hashing: XORâ€‘based Zobrist, incremental updates, 128â€‘bit keys.

---
ğŸ“¤ Training data export (JSONL)

Overview
- Export solver-labelled states to JSONL for training and analysis.
- Two export modes:
  - --export-mode trajectory (default): emits a single 9-state principal-variation trajectory per sampled game.
  - --export-mode full: emits the entire reachable state space for one sampled hand pair (exhaustive).
- Deterministic by design: same seed+flags â†’ identical output (including elements layout, off-PV behaviour, and policy/value targets).

Core flags
- Output and mode
  - --export PATH                JSONL output file
  - --export-mode trajectory|full
  - --games N                    number of games (trajectory mode only; full mode ignores this)
- Sampling and determinism
  - --seed U64                   master seed; controls hand sampling, per-ply RNG (off-PV), and MCTS rollouts
  - --hand-strategy random|stratified
  - --rules elemental,same,plus,same_wall (or 'none')
  - --elements none|random       element layout (random is deterministic per game)
- Policy and value targets
  - --policy-format onehot|mcts
  - --mcts-rollouts N            rollouts used when --policy-format mcts (default 100)
  - --value-mode winloss|margin
- Off-PV stepping (trajectory mode only)
  - --off-pv-rate FLOAT [0..1]   per-game probability of off-PV stepping (0 disables)
  - --off-pv-strategy random|weighted|mcts
    - random: uniform over legal non-PV moves; if no alternative, fall back to PV.
    - weighted: softmax over root child negamax Q values; PV mass set to 0, renormalised, then sampled.
    - mcts: reuse root MCTS distribution when policy_format=mcts; otherwise compute root MCTS with --mcts-rollouts and sample after zeroing PV.

Parallel export (worker + writer) â€” TASK T7
- The export pipeline now supports an explicit worker + writer model (replacing the previous Rayon pool).
- New CLI flag: --threads N (default: available_parallelism()-1). Workers claim game indices via an atomic counter and operate deterministically.
- Worker behavior:
  - Deterministically sample hands/elements from the master seed and game id.
  - For each game produce 9 JSONL lines by running full-depth solver searches (using search_root_with_children() for policy Q values).
  - Send (game_id, Vec<String>) to the single writer thread.
- Writer behavior:
  - Single dedicated writer receives per-game line blocks, buffers out-of-order arrivals, and writes games in strict increasing game_id order to maintain byte-for-byte determinism across different thread counts.
- Determinism guarantees:
  - With identical seed + flags the JSONL output is byte-for-byte identical whether --threads=1 or --threads>1.
  - Tests: --games 10 produces 90 lines; output compared equal across thread counts.
- Progress:
  - The trajectory export includes an indicatif ProgressBar with a background updater that reports states/sec, nodes/sec, ETA, and policy info.

Determinisim specifics
- Trajectory mode:
  - Labels (policy/value) are computed from full-depth perfect play at each ply; labels remain PV-optimal regardless of stepping strategy.
  - Off-PV stepping uses a per-ply RNG derived from (seed, game_id, turn) for deterministic sampling.
- Full mode:
  - Enumeration is deterministic. For optional MCTS policy distributions in full mode, rollout RNG derives from (seed XOR state_zobrist) to preserve traversal-order invariance.
- With identical seeds and flags, the exported JSONL is byte-for-byte identical.

CLI examples
- Trajectory export (onehot), 10 games, no elements:
  - target/release/precompute --export export.jsonl --export-mode trajectory --games 10 --seed 42 --hand-strategy random --rules none --elements none --policy-format onehot --value-mode winloss
- Trajectory export (MCTS policy, 256 rollouts), stratified hands, Elemental+Same, margin values:
  - target/release/precompute --export export_mcts.jsonl --export-mode trajectory --games 10 --seed 42 --hand-strategy stratified --rules elemental,same --elements random --policy-format mcts --mcts-rollouts 256 --value-mode margin
- Trajectory export with off-PV weighted stepping @ 20% games:
  - target/release/precompute --export export_weighted.jsonl --export-mode trajectory --games 100 --seed 123 --hand-strategy random --rules none --elements none --policy-format onehot --off-pv-rate 0.2 --off-pv-strategy weighted
- Trajectory export with off-PV mcts stepping, reusing policy distribution:
  - target/release/precompute --export export_mcts_offpv.jsonl --export-mode trajectory --games 100 --seed 123 --hand-strategy stratified --rules elemental,same --elements random --policy-format mcts --mcts-rollouts 128 --off-pv-rate 0.2 --off-pv-strategy mcts
- Full stateâ€‘space export (exhaustive) for one sampled hand pair:
  - target/release/precompute --export export_full.jsonl --export-mode full --seed 42 --hand-strategy random --rules none --elements none --policy-format onehot --value-mode winloss

JSONL schema
Each line contains one state emitted before a move is taken.

{
  "game_id": 12,                // sequential per sampled game (trajectory); 0 in full mode
  "state_idx": 0,               // 0..8 within a trajectory (equals "turn")
  "board": [
    {"cell":0,"card_id":12,"owner":"A","element":"F"}, // element present only when rules.elemental=true; otherwise omitted
    {"cell":1,"card_id":null,"owner":null,"element":null},
    {"cell":2,"card_id":null,"owner":null,"element":"W"}
  ],
  "hands": {
    "A": [34,56,78,90,11],      // shrinking lists (5 â†’ 0 across a trajectory)
    "B": [22,33,44,55,66]
  },
  "to_move": "A",
  "turn": 0,
  "rules": {
    "elemental": true,
    "same": true,
    "plus": false,
    "same_wall": false
  },

  // Policy target semantics:
  // onehot â†’ single best move object:
  //   "policy_target": {"card_id":34,"cell":0}
  // mcts â†’ distribution over legal moves:
  //   "policy_target": {"34-0":0.7,"56-1":0.3}
  "policy_target": {"34-0": 0.7, "56-1": 0.3},

  // Value target semantics controlled by --value-mode:
  // - winloss â†’ {-1,0,+1} from side-to-move perspective (sign of solver value)
  // - margin  â†’ integer margin A_cards âˆ’ B_cards (A-perspective)
  "value_target": 1,
  "value_mode": "winloss",

  // Off-principal-variation sampling flag; true for all lines of off-PV games, false otherwise
  "off_pv": false,

  // 128-bit Zobrist hash of the state (hex string)
  "state_hash": "a1b2c3d4e5f6a1b2c3d4e5f6a1b2c3d4"
}

Export semantics

State timing
- Each JSONL line represents the state before a move is made.
- At turn = t, the board has t occupied cells; the side in to_move will play their (t+1)-th card next.
- Hands shrink accordingly; at the final turn = 8 there is still one card left (and a policy_target showing where it will be placed).

Policy targets
- --policy-format onehot:
  - Exported as a single best-move object:
    "policy_target": {"card_id": 34, "cell": 0}
- --policy-format mcts:
  - Exported as a {"cardId-cell": prob} map over legal moves:
    "policy_target": {"34-0": 0.7, "56-1": 0.3}
- Terminal states:
  - onehot â†’ policy_target omitted (null).
  - mcts â†’ empty map {}.

Off-PV stepping (trajectory mode)
- Labels remain PV-optimal: policy/value targets reflect perfect play; stepping only affects which move is played next to advance the trajectory.
- Per-game activation: with probability --off-pv-rate a game uses off-PV stepping for all its plies.
- Strategies:
  - random:
    - Uniform over legal non-PV moves. If no alternative exists, fall back to the PV move (or None at terminal).
  - weighted:
    - Use root child negamax values (Q). Compute softmax(Q), set PV move probability to 0, renormalise, sample with a deterministic RNG derived from (seed, game_id, turn).
    - If no alternative exists, fall back to PV.
  - mcts:
    - If --policy-format mcts, reuse the root MCTS distribution computed for policy_target; otherwise compute a fresh root MCTS with --mcts-rollouts (deterministic per-ply seed).
    - Set PV probability to 0, renormalise, sample deterministically.
    - Fallback: if the PV move had all probability mass (sum of non-PV probs == 0), select the highest-probability non-PV move deterministically.

Value targets
- --value-mode winloss:
  - value âˆˆ {-1, 0, +1}, always from side-to-move perspective.
    - +1 = side-to-move wins under perfect play
    - 0  = draw
    - âˆ’1 = side-to-move loses
- --value-mode margin:
  - integer final score difference (A_cards âˆ’ B_cards), always from Aâ€™s perspective, independent of who is to move.
- Values are computed by solving the full remaining depth at each ply, so they match final outcomes.

Elements
- When rules.elemental = true, each board cell includes an "element" field with one of: F, I, T, W, E, P, H, L.
- Otherwise the "element" field is omitted.

Metadata
- game_id: sequential per sampled game (trajectory); 0 in full mode.
- state_idx: 0..8 within a trajectory (equals turn).
- off_pv: true for all lines of off-PV games; false otherwise.
- state_hash: 128â€‘bit Zobrist hash of the state, hex string.

ğŸ•¹ï¸ Rules implemented

- Basic capture: placed card flips weaker adjacent opponents.
- Elemental: +1/âˆ’1 adjustments based on cell element.
- Same: 2+ equalities trigger flips.
- Same Wall: walls count as 10 for Same.
- Plus: equal sums trigger flips.
- Combo: cascades apply Basic rule only.

---
âš¡ Performance & determinism

- Move ordering: Corners > Edges > Center, then cell index, then card id.
- Cascade order: BFS queue, ascending indices.
- TT: fixedâ€‘size array, depthâ€‘preferred replacement, full key verification.
- Progress: MultiProgress bars, states/sec, nodes/sec.
- Determinism: same state â†’ same result, reproducible DBs.

---
ğŸ”® Roadmap

Nearâ€‘term

- Query API: CLI/HTTP service for instant bestâ€‘move lookups from solved DBs.
- Analysis experiments:
	- Firstâ€‘player advantage quantification.
	- Card/hand Elo ratings.
	- Elemental variance studies.
	- Rule interaction balance.

Midâ€‘term

- AI/ML integration:
	- NPZ export option for compact storage.
	- PyTorch Dataset utilities to load JSONL â†’ tensors.
	- Train small policy/value nets for lightweight move suggestion.
	- Reinforced selfâ€‘play (AlphaZeroâ€‘lite) to refine policies.
- Imperfectâ€‘info play:
	- Expectimax/MCTS with hidden hands.
	- Opponent modelling.

Longâ€‘term

- Superhuman assistant:
	- Instant move suggestions in Open and Closed formats.
	- Configurable playstyles: Optimal, Robust, Exploitative.
	- Humanâ€‘like play modes for fun.
- Metaâ€‘analysis:
	- â€œBest handâ€ search for each ruleset.
	- Card tier lists and balance insights.
	- Quantitative impact of Elemental RNG.

---
ğŸ§ª Testing scope

- Engine tests: rule correctness, determinism, allocationâ€‘free hot paths.
- Solver tests: terminal values, PV determinism.
- Rule edge tests: Same, Plus, Same Wall, Combo cascades.
- Persistence tests: batch flush, compression, checkpoint/resume determinism.

---
ğŸ“œ License


The code in this repository is provided under an open license (TBD).
Triple Triad belongs to Square Enix (Final Fantasy VIII). This is a cleanâ€‘room reimplementation for research and educational purposes.

---
ğŸ™ Acknowledgements

- FF8â€™s Triple Triad ruleset by Square Enix.
- Chess/Go engine design patterns (negamax, Î±Î², Zobrist, TT).
- OpenAI GPTâ€‘5 + Kilo Code for collaborative design and implementation.